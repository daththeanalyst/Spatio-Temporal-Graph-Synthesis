{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 25px; background-color: #1a1a2e; border-radius: 12px; border: 2px solid #e94560;\">\n",
    "    <h1 style=\"color: #e94560; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Predictive Site Selection Model</h1>\n",
    "    <h2 style=\"color: #0f3460;\">Binary Classification on H3 Spatial Hexagons — Camden Specialty Coffee</h2>\n",
    "    <p style=\"color: #eee; font-size: 1.1em;\">\n",
    "        Predicting specialty coffee shop suitability from multi-modal geospatial features:<br>\n",
    "        <strong>LandScan</strong> population rasters, <strong>ONS Census 2021</strong> demographics, and <strong>NetworkX</strong> graph centrality.\n",
    "    </p>\n",
    "    <hr style=\"border-color: #e94560;\">\n",
    "    <div style=\"display: flex; justify-content: space-between; color: #aaa;\">\n",
    "        <span>London Borough of Camden</span>\n",
    "        <span>MSc Business Analytics | 2026</span>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px; padding: 15px; background-color: #eaf2f8; border-radius: 8px; border-left: 5px solid #2e86c1;\">\n",
    "    <h2 style=\"color: #1a5276;\">Pipeline Overview</h2>\n",
    "    <p>This notebook is <strong>self-contained</strong>. It re-derives all features from raw data sources so it can run independently of Notebooks 01–03. The pipeline:</p>\n",
    "    <ol>\n",
    "        <li><strong>Data Assembly</strong>: Load H3 grid, LandScan raster, OSM POIs, and 3 ONS Census CSVs.</li>\n",
    "        <li><strong>Feature Engineering</strong>: 12 features across 4 modalities (footfall, demographics, graph centrality, POI ecosystem).</li>\n",
    "        <li><strong>Target Definition</strong>: Binary — <code>has_coffee_shop</code> (1/0) per hexagon.</li>\n",
    "        <li><strong>Spatial Cross-Validation</strong>: H3 parent-cell block CV to prevent spatial leakage.</li>\n",
    "        <li><strong>Model Comparison</strong>: Logistic Regression vs. Random Forest vs. XGBoost.</li>\n",
    "        <li><strong>Hyperparameter Tuning</strong>: GridSearchCV with spatial folds.</li>\n",
    "        <li><strong>Business Insight</strong>: Extract False Positives as site recommendations.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 0: Environment & Imports\n",
    "# ============================================================\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import h3\n",
    "import rasterio\n",
    "import rasterstats\n",
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydeck as pdk\n",
    "from shapely.geometry import Polygon, Point\n",
    "\n",
    "# ML Stack\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score,\n",
    "    roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Directory scaffold\n",
    "for d in ['data/raw', 'data/processed', 'data/outputs']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"H3 version: {h3.__version__}\")\n",
    "print(f\"Environment ready. Random state: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #e94560; border-bottom: 2px solid #e94560; padding-bottom: 10px;\">Section 1: Data Preparation — The Feature Matrix</h2>\n",
    "    <p>We assemble a single flat <code>DataFrame</code> by joining three data modalities onto the H3 hexagonal grid.</p>\n",
    "    <table style=\"width:100%; border-collapse: collapse; margin-top: 10px; font-size: 0.95em;\">\n",
    "        <tr style=\"background: #16213e; color: white;\">\n",
    "            <th style=\"padding: 8px; text-align: left;\">Modality</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Source</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Features</th>\n",
    "        </tr>\n",
    "        <tr style=\"background: #f8f9fa;\"><td style=\"padding: 8px;\">Footfall</td><td>LandScan Raster (zonal stats)</td><td><code>population</code></td></tr>\n",
    "        <tr><td style=\"padding: 8px;\">Demographics</td><td>ONS Census 2021 (Digimap)</td><td><code>employed_total_perc</code>, <code>age_16_to_34_perc</code>, <code>level4_perc</code>, <code>retired_perc</code>, <code>no_qualifications_perc</code></td></tr>\n",
    "        <tr style=\"background: #f8f9fa;\"><td style=\"padding: 8px;\">Graph Centrality</td><td>NetworkX on H3 adjacency</td><td><code>degree_centrality</code>, <code>betweenness_centrality</code>, <code>closeness_centrality</code>, <code>clustering_coeff</code></td></tr>\n",
    "        <tr><td style=\"padding: 8px;\">POI Ecosystem</td><td>OSMnx spatial join</td><td><code>n_synergy</code>, <code>n_anchors</code> (competitors excluded — they <em>are</em> the target)</td></tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.1: Generate H3 Grid over Camden\n",
    "# ============================================================\n",
    "PLACE = \"London Borough of Camden\"\n",
    "RESOLUTION = 9  # ~174m edge length — walking scale\n",
    "\n",
    "# Fetch Camden boundary\n",
    "boundary = ox.geocode_to_gdf(PLACE)\n",
    "boundary_wgs84 = boundary.to_crs(epsg=4326)\n",
    "poly = boundary_wgs84.geometry.iloc[0]\n",
    "\n",
    "# H3 v4: polygon_to_cells\n",
    "outer_coords = [(lat, lng) for lng, lat in poly.exterior.coords]\n",
    "holes = [[(lat, lng) for lng, lat in ring.coords] for ring in poly.interiors]\n",
    "h3_poly = h3.LatLngPoly(outer_coords, *holes)\n",
    "hex_ids = h3.polygon_to_cells(h3_poly, RESOLUTION)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "def h3_to_shapely(cell_id):\n",
    "    coords = h3.cell_to_boundary(cell_id)\n",
    "    return Polygon([(lng, lat) for lat, lng in coords])\n",
    "\n",
    "hex_polygons = [h3_to_shapely(h) for h in hex_ids]\n",
    "h3_grid = gpd.GeoDataFrame(\n",
    "    {'h3_index': list(hex_ids)},\n",
    "    geometry=hex_polygons,\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "\n",
    "print(f\"H3 Grid: {len(h3_grid)} hexagons at Resolution {RESOLUTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.2: Footfall — LandScan Raster Enrichment\n",
    "# ============================================================\n",
    "raster_path = \"landscan-mosaic-unitedkingdom-v1.tif\"\n",
    "\n",
    "with rasterio.open(raster_path) as src:\n",
    "    print(f\"Raster CRS: {src.crs} | Resolution: {src.res}\")\n",
    "\n",
    "# Zonal stats: sum population pixels under each hex\n",
    "stats = rasterstats.zonal_stats(h3_grid, raster_path, stats=['sum'], nodata=-999)\n",
    "h3_grid['population'] = [s['sum'] if s['sum'] is not None else 0 for s in stats]\n",
    "\n",
    "print(f\"Total LandScan population across Camden hexes: {h3_grid['population'].sum():,.0f}\")\n",
    "print(f\"Max hex population: {h3_grid['population'].max():,.0f}\")\n",
    "print(f\"Hexes with zero population: {(h3_grid['population'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.3: POI Ecosystem — OSMnx Fetch & Categorisation\n",
    "# ============================================================\n",
    "tags = {\n",
    "    'amenity': ['cafe', 'coffee_shop', 'gym', 'university', 'office',\n",
    "                'library', 'leisure_centre'],\n",
    "    'leisure': ['fitness_centre', 'sports_centre'],\n",
    "    'shop': ['bakery', 'supermarket'],\n",
    "    'public_transport': ['station']\n",
    "}\n",
    "\n",
    "pois_raw = ox.features_from_place(PLACE, tags)\n",
    "pois_raw['geometry'] = pois_raw.to_crs(epsg=27700).centroid\n",
    "pois_raw = pois_raw.set_crs(epsg=27700)\n",
    "pois = pois_raw[pois_raw.geometry.type == 'Point'].copy()\n",
    "pois = pois.to_crs(epsg=4326)  # match h3_grid CRS for sjoin\n",
    "\n",
    "# Categorise by business role\n",
    "def categorize(row):\n",
    "    amenity = row.get('amenity', '')\n",
    "    leisure = row.get('leisure', '')\n",
    "    transport = row.get('public_transport', '')\n",
    "    if amenity in ('cafe', 'coffee_shop'):\n",
    "        return 'Competitor'\n",
    "    if amenity in ('gym', 'university', 'office', 'library', 'leisure_centre') \\\n",
    "       or leisure in ('fitness_centre', 'sports_centre'):\n",
    "        return 'Synergy'\n",
    "    if transport == 'station':\n",
    "        return 'Anchor'\n",
    "    return 'Other'\n",
    "\n",
    "pois['role'] = pois.apply(categorize, axis=1)\n",
    "\n",
    "print(f\"POIs fetched: {len(pois)}\")\n",
    "print(pois['role'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.4: Spatial Join — POI Counts per Hexagon\n",
    "# ============================================================\n",
    "pois_in_hex = gpd.sjoin(pois, h3_grid[['h3_index', 'geometry']], how='inner', predicate='within')\n",
    "\n",
    "# Pivot by role\n",
    "role_counts = pois_in_hex.groupby(['h3_index', 'role']).size().unstack(fill_value=0)\n",
    "\n",
    "# Ensure all role columns exist\n",
    "for col in ['Competitor', 'Synergy', 'Anchor', 'Other']:\n",
    "    if col not in role_counts.columns:\n",
    "        role_counts[col] = 0\n",
    "\n",
    "role_counts = role_counts.rename(columns={\n",
    "    'Competitor': 'n_competitors',\n",
    "    'Synergy': 'n_synergy',\n",
    "    'Anchor': 'n_anchors',\n",
    "    'Other': 'n_other'\n",
    "})\n",
    "\n",
    "h3_grid = h3_grid.merge(role_counts, on='h3_index', how='left')\n",
    "for col in ['n_competitors', 'n_synergy', 'n_anchors', 'n_other']:\n",
    "    h3_grid[col] = h3_grid[col].fillna(0).astype(int)\n",
    "\n",
    "print(f\"Competitors: {h3_grid['n_competitors'].sum()} | Synergy: {h3_grid['n_synergy'].sum()} | Anchors: {h3_grid['n_anchors'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.5: Demographics — ONS Census Enrichment\n",
    "# ============================================================\n",
    "# Load all 3 census CSVs\n",
    "econ_df = pd.read_csv('ons-economic-ew-2021_6304504/ons-economic-ew-2021.csv')\n",
    "age_df  = pd.read_csv('ons-age-ew-2021_6304503/ons-age-ew-2021.csv')\n",
    "qual_df = pd.read_csv('ons-qualifications-ew-2021_6304505/ons-qualifications-ew-2021.csv')\n",
    "\n",
    "# Merge on geog_code\n",
    "census = econ_df[['geog_code', 'centroid_x', 'centroid_y', 'denom_total',\n",
    "                   'employed_total_perc', 'retired_perc', 'unemployed_perc']].copy()\n",
    "census = census.merge(\n",
    "    age_df[['geog_code', 'age_16_to_34_perc', 'age_65_plus_perc']], on='geog_code', how='left'\n",
    ")\n",
    "census = census.merge(\n",
    "    qual_df[['geog_code', 'level4_perc', 'no_qualifications_perc']], on='geog_code', how='left'\n",
    ")\n",
    "\n",
    "# Convert to GeoDataFrame (centroids are already BNG EPSG:27700)\n",
    "geometry = [Point(xy) for xy in zip(census['centroid_x'], census['centroid_y'])]\n",
    "census_gdf = gpd.GeoDataFrame(census, geometry=geometry, crs='EPSG:27700')\n",
    "census_gdf = census_gdf.to_crs(epsg=4326)  # match h3_grid\n",
    "\n",
    "# Spatial join: nearest census OA centroid -> hex\n",
    "census_in_hex = gpd.sjoin_nearest(\n",
    "    census_gdf, h3_grid[['h3_index', 'geometry']],\n",
    "    how='left', max_distance=0.005\n",
    ")\n",
    "\n",
    "# Population-weighted mean of demographics per hex\n",
    "demo_cols = ['employed_total_perc', 'age_16_to_34_perc', 'level4_perc',\n",
    "             'retired_perc', 'no_qualifications_perc']\n",
    "\n",
    "def weighted_mean(group, col):\n",
    "    w = group['denom_total']\n",
    "    return (group[col] * w).sum() / w.sum() if w.sum() > 0 else 0\n",
    "\n",
    "hex_demos = []\n",
    "for h3_id, group in census_in_hex.groupby('h3_index'):\n",
    "    row = {'h3_index': h3_id}\n",
    "    for col in demo_cols:\n",
    "        row[col] = weighted_mean(group, col)\n",
    "    hex_demos.append(row)\n",
    "\n",
    "demo_df = pd.DataFrame(hex_demos)\n",
    "h3_grid = h3_grid.merge(demo_df, on='h3_index', how='left')\n",
    "for col in demo_cols:\n",
    "    h3_grid[col] = h3_grid[col].fillna(0)\n",
    "\n",
    "print(f\"Census demographics joined. Coverage: {len(demo_df)}/{len(h3_grid)} hexes\")\n",
    "print(f\"  Mean Level 4%: {h3_grid['level4_perc'].mean():.1f}%\")\n",
    "print(f\"  Mean Age 16-34%: {h3_grid['age_16_to_34_perc'].mean():.1f}%\")\n",
    "print(f\"  Mean Employment%: {h3_grid['employed_total_perc'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #0f3460; border-bottom: 2px solid #0f3460; padding-bottom: 10px;\">Section 1.6: Graph Centrality Features</h2>\n",
    "    <p>We construct the H3 adjacency graph and compute four centrality metrics per hexagon:</p>\n",
    "    <ul>\n",
    "        <li><strong>Degree Centrality</strong>: $C_D(v) = \\frac{\\deg(v)}{N-1}$ — connectivity (number of neighbours).</li>\n",
    "        <li><strong>Betweenness Centrality</strong>: $C_B(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}$ — bridging position on shortest paths.</li>\n",
    "        <li><strong>Closeness Centrality</strong>: $C_C(v) = \\frac{N-1}{\\sum_{u} d(v,u)}$ — accessibility to all other hexes.</li>\n",
    "        <li><strong>Clustering Coefficient</strong>: $C_{clust}(v) = \\frac{2 T(v)}{\\deg(v)(\\deg(v)-1)}$ — neighbourhood cohesion.</li>\n",
    "    </ul>\n",
    "    <p>These capture a hexagon's <em>structural role</em> in the urban network — independent of its content.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.6: Graph Centrality Features\n",
    "# ============================================================\n",
    "G = nx.Graph()\n",
    "hex_set = set(h3_grid['h3_index'])\n",
    "\n",
    "for h3_id in hex_set:\n",
    "    G.add_node(h3_id)\n",
    "\n",
    "for h3_id in hex_set:\n",
    "    for nb in h3.grid_disk(h3_id, 1):\n",
    "        if nb != h3_id and nb in hex_set:\n",
    "            G.add_edge(h3_id, nb)\n",
    "\n",
    "# Compute centrality metrics\n",
    "h3_grid['degree_centrality'] = h3_grid['h3_index'].map(nx.degree_centrality(G))\n",
    "h3_grid['betweenness_centrality'] = h3_grid['h3_index'].map(nx.betweenness_centrality(G))\n",
    "h3_grid['closeness_centrality'] = h3_grid['h3_index'].map(nx.closeness_centrality(G))\n",
    "h3_grid['clustering_coeff'] = h3_grid['h3_index'].map(nx.clustering(G))\n",
    "\n",
    "print(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "print(f\"Mean degree centrality: {h3_grid['degree_centrality'].mean():.4f}\")\n",
    "print(f\"Mean betweenness centrality: {h3_grid['betweenness_centrality'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #e94560; border-bottom: 2px solid #e94560; padding-bottom: 10px;\">Section 2: Target Variable — Binary Coffee Shop Presence</h2>\n",
    "    <p>We define the prediction target as:</p>\n",
    "    <p style=\"text-align: center; font-size: 1.2em;\">$$y_H = \\begin{cases} 1 & \\text{if hexagon } H \\text{ contains } \\geq 1 \\text{ cafe or coffee\\_shop} \\\\ 0 & \\text{otherwise} \\end{cases}$$</p>\n",
    "    <p><strong>Class imbalance is expected</strong>: most hexagons will not contain a coffee shop. We handle this with <code>class_weight='balanced'</code> (LR, RF) and <code>scale_pos_weight</code> (XGBoost).</p>\n",
    "    <div style=\"background: #fff3cd; padding: 12px; border-radius: 5px; border-left: 4px solid #ffc107; margin-top: 10px;\">\n",
    "        <strong>Leakage Warning:</strong> The <code>n_competitors</code> column counts cafes/coffee shops per hex — this directly encodes the target.\n",
    "        It <strong>must be excluded</strong> from the feature matrix <code>X</code>. We retain <code>n_synergy</code> and <code>n_anchors</code>\n",
    "        as legitimate predictors (synergy nodes attract coffee shops, not vice versa).\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 2: Target Definition & Feature Matrix\n",
    "# ============================================================\n",
    "\n",
    "# Binary target\n",
    "h3_grid['has_coffee_shop'] = (h3_grid['n_competitors'] >= 1).astype(int)\n",
    "\n",
    "print(\"Target distribution:\")\n",
    "print(h3_grid['has_coffee_shop'].value_counts())\n",
    "print(f\"Positive class rate: {h3_grid['has_coffee_shop'].mean():.1%}\")\n",
    "\n",
    "# === Feature set (LEAKAGE GUARD: n_competitors excluded) ===\n",
    "FEATURE_COLS = [\n",
    "    # Footfall\n",
    "    'population',\n",
    "    # Demographics\n",
    "    'employed_total_perc',\n",
    "    'age_16_to_34_perc',\n",
    "    'level4_perc',\n",
    "    'retired_perc',\n",
    "    'no_qualifications_perc',\n",
    "    # Graph Centrality\n",
    "    'degree_centrality',\n",
    "    'betweenness_centrality',\n",
    "    'closeness_centrality',\n",
    "    'clustering_coeff',\n",
    "    # POI Ecosystem (synergy + anchors ONLY)\n",
    "    'n_synergy',\n",
    "    'n_anchors',\n",
    "]\n",
    "\n",
    "TARGET_COL = 'has_coffee_shop'\n",
    "\n",
    "X = h3_grid[FEATURE_COLS].copy()\n",
    "y = h3_grid[TARGET_COL].copy()\n",
    "\n",
    "# Leakage assertion\n",
    "assert 'n_competitors' not in X.columns, \"LEAKAGE: n_competitors found in feature set!\"\n",
    "\n",
    "print(f\"\\nFeature matrix X: {X.shape}\")\n",
    "print(f\"Features ({len(FEATURE_COLS)}): {FEATURE_COLS}\")\n",
    "print(f\"\\nMissing values per feature:\")\n",
    "print(X.isnull().sum().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #0f3460; border-bottom: 2px solid #0f3460; padding-bottom: 10px;\">Section 3: Exploratory Data Analysis & Leakage Audit</h2>\n",
    "    <p>Before modelling, we verify:</p>\n",
    "    <ol>\n",
    "        <li><strong>Feature distributions</strong>: Zero variance, extreme skew, or missing values?</li>\n",
    "        <li><strong>Multicollinearity</strong>: Variance Inflation Factors (VIF) — flag features with VIF &gt; 10.</li>\n",
    "        <li><strong>Spatial autocorrelation</strong>: Visual confirmation that the target is spatially clustered (motivating Spatial CV).</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 3a: Correlation Heatmap\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "corr = X.join(y).corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(\n",
    "    corr, mask=mask, annot=True, fmt='.2f',\n",
    "    cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "    square=True, linewidths=0.5, ax=ax\n",
    ")\n",
    "ax.set_title('Feature Correlation Matrix (incl. Target)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/outputs/eda_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Flag high correlations with target\n",
    "target_corr = corr[TARGET_COL].drop(TARGET_COL).sort_values(ascending=False)\n",
    "print(\"Feature correlations with target (has_coffee_shop):\")\n",
    "print(target_corr.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 3b: Feature Distributions by Target Class\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "key_features = ['population', 'level4_perc', 'age_16_to_34_perc',\n",
    "                'betweenness_centrality', 'n_synergy', 'n_anchors']\n",
    "\n",
    "for ax, feat in zip(axes.ravel(), key_features):\n",
    "    plot_df = X[[feat]].copy()\n",
    "    plot_df['target'] = y.values\n",
    "    sns.boxplot(data=plot_df, x='target', y=feat, ax=ax,\n",
    "                palette={0: '#3498db', 1: '#e74c3c'})\n",
    "    ax.set_title(feat, fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('has_coffee_shop')\n",
    "\n",
    "plt.suptitle('Feature Distributions by Target Class', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/outputs/eda_boxplots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 3c: Variance Inflation Factor (Multicollinearity)\n",
    "# ============================================================\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# VIF requires no NaN and no constant columns\n",
    "X_vif = X.fillna(0).copy()\n",
    "# Remove zero-variance columns for VIF calculation\n",
    "X_vif = X_vif.loc[:, X_vif.std() > 0]\n",
    "\n",
    "vif_data = pd.DataFrame({\n",
    "    'Feature': X_vif.columns,\n",
    "    'VIF': [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "}).sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"Variance Inflation Factors:\")\n",
    "print(vif_data.to_string(index=False))\n",
    "print(f\"\\nFeatures with VIF > 10 (potential multicollinearity): \"\n",
    "      f\"{list(vif_data[vif_data['VIF'] > 10]['Feature'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #c0392b; border-bottom: 2px solid #c0392b; padding-bottom: 10px;\">Section 4: Spatial Cross-Validation Strategy</h2>\n",
    "    <p><strong>Problem:</strong> Standard <code>KFold</code> randomly assigns hexagons to train/test folds.\n",
    "    Adjacent hexes share similar features due to spatial autocorrelation\n",
    "    (<em>Tobler's First Law of Geography</em>: \"near things are more related than distant things\").\n",
    "    Random splits leak spatial signal, inflating performance metrics by 5–15%.</p>\n",
    "    \n",
    "    <p><strong>Solution:</strong> <strong>Spatial Block Cross-Validation</strong> using the H3 hierarchy.\n",
    "    We group Resolution-9 hexagons by their Resolution-5 parent cell (∼10 km² blocks).\n",
    "    All hexes sharing a parent are assigned to the same fold, preserving spatial integrity.</p>\n",
    "    \n",
    "    <p style=\"text-align: center; font-size: 1.1em;\">$$\\text{fold}(H) = \\text{h3\\_cell\\_to\\_parent}(H, \\text{res}=5) \\bmod k$$</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 4: Spatial Block Cross-Validation (H3 Parent-Cell)\n",
    "# ============================================================\n",
    "\n",
    "class SpatialKFold:\n",
    "    \"\"\"Spatial block cross-validation using H3 parent-cell partitioning.\n",
    "\n",
    "    Groups H3 Res-9 hexagons by their Res-5 parent cell, then assigns\n",
    "    each spatial block to a fold. Ensures geographically proximate\n",
    "    hexagons never appear in both train and test simultaneously.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, parent_resolution=5):\n",
    "        self.n_splits = n_splits\n",
    "        self.parent_resolution = parent_resolution\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        parents = groups.apply(\n",
    "            lambda h: h3.cell_to_parent(h, self.parent_resolution)\n",
    "        )\n",
    "        unique_parents = parents.unique()\n",
    "        parent_to_fold = {\n",
    "            p: i % self.n_splits\n",
    "            for i, p in enumerate(unique_parents)\n",
    "        }\n",
    "        fold_assignments = parents.map(parent_to_fold)\n",
    "\n",
    "        for fold_idx in range(self.n_splits):\n",
    "            test_mask = fold_assignments == fold_idx\n",
    "            train_idx = np.where(~test_mask)[0]\n",
    "            test_idx = np.where(test_mask)[0]\n",
    "            yield train_idx, test_idx\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "\n",
    "spatial_cv = SpatialKFold(n_splits=5, parent_resolution=5)\n",
    "\n",
    "# Verify fold sizes\n",
    "print(\"Spatial CV Fold Sizes:\")\n",
    "for i, (train_idx, test_idx) in enumerate(\n",
    "    spatial_cv.split(X, y, groups=h3_grid['h3_index'])\n",
    "):\n",
    "    print(f\"  Fold {i+1}: train={len(train_idx)}, test={len(test_idx)}, \"\n",
    "          f\"test_positive_rate={y.iloc[test_idx].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #117a65; border-bottom: 2px solid #117a65; padding-bottom: 10px;\">Section 5: Baseline Model — Logistic Regression</h2>\n",
    "    <p>Logistic Regression serves as our <strong>interpretability baseline</strong>. Its linear decision boundary provides\n",
    "    directly readable coefficients — the sign and magnitude of each feature's effect on coffee shop presence.</p>\n",
    "    <p>We apply <code>StandardScaler</code> (required for LR convergence) and <code>class_weight='balanced'</code>\n",
    "    to handle the expected class imbalance.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 5: Baseline — Logistic Regression\n",
    "# ============================================================\n",
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(\n",
    "        class_weight='balanced', max_iter=1000, random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "lr_scores = cross_val_score(\n",
    "    lr_pipe, X, y,\n",
    "    cv=spatial_cv.split(X, y, groups=h3_grid['h3_index']),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f\"Logistic Regression (Spatial CV):\")\n",
    "print(f\"  ROC-AUC: {lr_scores.mean():.3f} \\u00b1 {lr_scores.std():.3f}\")\n",
    "print(f\"  Per-fold: {[f'{s:.3f}' for s in lr_scores]}\")\n",
    "\n",
    "# Fit on full data for coefficient inspection\n",
    "lr_pipe.fit(X, y)\n",
    "lr_coefs = pd.Series(\n",
    "    lr_pipe.named_steps['model'].coef_[0],\n",
    "    index=FEATURE_COLS\n",
    ").sort_values()\n",
    "\n",
    "print(f\"\\nLogistic Regression Coefficients (standardised):\")\n",
    "print(lr_coefs.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #884ea0; border-bottom: 2px solid #884ea0; padding-bottom: 10px;\">Section 6: Model Comparison — Random Forest & XGBoost</h2>\n",
    "    <p>We compare three models spanning the complexity spectrum:</p>\n",
    "    <table style=\"width:100%; border-collapse: collapse; margin-top: 10px;\">\n",
    "        <tr style=\"background: #16213e; color: white;\">\n",
    "            <th style=\"padding: 8px;\">Model</th><th>Type</th><th>Class Imbalance Handling</th><th>Rationale</th>\n",
    "        </tr>\n",
    "        <tr><td style=\"padding: 8px;\">Logistic Regression</td><td>Linear</td><td><code>class_weight='balanced'</code></td><td>Interpretable baseline</td></tr>\n",
    "        <tr style=\"background: #f8f9fa;\"><td style=\"padding: 8px;\">Random Forest</td><td>Bagged ensemble</td><td><code>class_weight='balanced'</code></td><td>Non-linear, robust to outliers</td></tr>\n",
    "        <tr><td style=\"padding: 8px;\">XGBoost</td><td>Boosted ensemble</td><td><code>scale_pos_weight</code></td><td>State-of-the-art tabular performance</td></tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SECTION 6: Model Comparison — LR vs RF vs XGBoost\n# ============================================================\nimbalance_ratio = (y == 0).sum() / (y == 1).sum()\n\nmodels = {\n    'Logistic Regression': Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', LogisticRegression(\n            class_weight='balanced', max_iter=1000, random_state=RANDOM_STATE\n        ))\n    ]),\n    'Random Forest': RandomForestClassifier(\n        n_estimators=200, class_weight='balanced',\n        random_state=RANDOM_STATE, n_jobs=-1\n    ),\n    'XGBoost': xgb.XGBClassifier(\n        n_estimators=200,\n        scale_pos_weight=imbalance_ratio,\n        eval_metric='logloss',\n        random_state=RANDOM_STATE\n    )\n}\n\nresults = {}\nfor name, model in models.items():\n    scores = cross_val_score(\n        model, X, y,\n        cv=spatial_cv.split(X, y, groups=h3_grid['h3_index']),\n        scoring='roc_auc'\n    )\n    results[name] = {\n        'mean_auc': scores.mean(),\n        'std_auc': scores.std(),\n        'scores': scores\n    }\n    print(f\"{name:25s} ROC-AUC = {scores.mean():.3f} \\u00b1 {scores.std():.3f}\")\n\n# Summary table\nresults_df = pd.DataFrame({\n    'Model': results.keys(),\n    'Mean AUC': [r['mean_auc'] for r in results.values()],\n    'Std AUC': [r['std_auc'] for r in results.values()]\n}).sort_values('Mean AUC', ascending=False)\n\nprint(f\"\\nModel Comparison Summary:\")\nresults_df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #d35400; border-bottom: 2px solid #d35400; padding-bottom: 10px;\">Section 7: Hyperparameter Tuning</h2>\n",
    "    <p>We tune the best-performing model using <code>GridSearchCV</code> with our spatial CV splitter.\n",
    "    The search space covers tree depth, learning rate, subsampling, and column sampling —\n",
    "    the most impactful XGBoost hyperparameters for tabular classification.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SECTION 7: Hyperparameter Tuning (XGBoost)\n# ============================================================\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.7, 0.9],\n    'colsample_bytree': [0.7, 1.0]\n}\n\nxgb_base = xgb.XGBClassifier(\n    scale_pos_weight=imbalance_ratio,\n    eval_metric='logloss',\n    random_state=RANDOM_STATE\n)\n\n# Pre-compute spatial CV splits for GridSearchCV\ncv_splits = list(spatial_cv.split(X, y, groups=h3_grid['h3_index']))\n\ngrid_search = GridSearchCV(\n    xgb_base, param_grid,\n    cv=cv_splits,\n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1,\n    refit=True\n)\n\ngrid_search.fit(X, y)\n\nprint(f\"\\nBest Parameters: {grid_search.best_params_}\")\nprint(f\"Best ROC-AUC (Spatial CV): {grid_search.best_score_:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #1a5276; border-bottom: 2px solid #1a5276; padding-bottom: 10px;\">Section 8: Evaluation Suite</h2>\n",
    "    <p>We present the full evaluation battery: ROC curves, confusion matrix, feature importance, and classification report.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SECTION 8a: ROC Curves — Out-of-Fold Predictions (Spatial CV)\n# ============================================================\nfrom sklearn.model_selection import cross_val_predict\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Materialise spatial CV splits once (reuse for all models)\ncv_splits = list(spatial_cv.split(X, y, groups=h3_grid['h3_index']))\n\nfor name, model in models.items():\n    # Out-of-fold predictions: each sample predicted only when in the test fold\n    y_prob_oof = cross_val_predict(\n        model, X, y, cv=cv_splits, method='predict_proba'\n    )[:, 1]\n    fpr, tpr, _ = roc_curve(y, y_prob_oof)\n    auc_val = roc_auc_score(y, y_prob_oof)\n    ax.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC={auc_val:.3f})')\n\n# Tuned XGBoost: build a fresh estimator with best params for OOF evaluation\nbest_params = grid_search.best_params_\ntuned_xgb = xgb.XGBClassifier(\n    **best_params,\n    scale_pos_weight=imbalance_ratio,\n    eval_metric='logloss',\n    random_state=RANDOM_STATE\n)\ny_prob_oof_best = cross_val_predict(\n    tuned_xgb, X, y, cv=cv_splits, method='predict_proba'\n)[:, 1]\nfpr_b, tpr_b, _ = roc_curve(y, y_prob_oof_best)\nauc_best = roc_auc_score(y, y_prob_oof_best)\nax.plot(fpr_b, tpr_b, linewidth=2.5, linestyle='--',\n        label=f'XGBoost Tuned (AUC={auc_best:.3f})')\n\nax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random Baseline')\nax.set_xlabel('False Positive Rate', fontsize=12)\nax.set_ylabel('True Positive Rate', fontsize=12)\nax.set_title('ROC Curves: Spatial CV Out-of-Fold Predictions', fontsize=14, fontweight='bold')\nax.legend(loc='lower right', fontsize=10)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('data/outputs/roc_curves.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SECTION 8b: Confusion Matrix — Out-of-Fold (Spatial CV)\n# ============================================================\n# Use OOF predictions from the tuned XGBoost for honest evaluation\ny_pred_oof = cross_val_predict(tuned_xgb, X, y, cv=cv_splits)\n\nfig, ax = plt.subplots(figsize=(6, 5))\ncm = confusion_matrix(y, y_pred_oof)\ndisp = ConfusionMatrixDisplay(cm, display_labels=['No Coffee (0)', 'Has Coffee (1)'])\ndisp.plot(cmap='Blues', ax=ax, values_format='d')\nax.set_title('Confusion Matrix \\u2014 Spatial CV Out-of-Fold', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig('data/outputs/confusion_matrix.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nClassification Report (Out-of-Fold, Spatial CV):\")\nprint(classification_report(y, y_pred_oof, target_names=['No Coffee', 'Has Coffee']))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 8c: Feature Importance — Tuned XGBoost\n",
    "# ============================================================\n",
    "importances = pd.Series(\n",
    "    best_model.feature_importances_, index=FEATURE_COLS\n",
    ").sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "importances.plot(\n",
    "    kind='barh', ax=ax,\n",
    "    color=['#e94560' if v > importances.median() else '#3498db' for v in importances]\n",
    ")\n",
    "ax.set_xlabel('Importance (Gain)', fontsize=12)\n",
    "ax.set_title('Feature Importance \\u2014 Tuned XGBoost', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=importances.median(), color='gray', linestyle='--', alpha=0.5, label='Median')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/outputs/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "print(importances.sort_values(ascending=False).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #27ae60; border-bottom: 2px solid #27ae60; padding-bottom: 10px;\">Section 9: Business Insight — Mining False Positives</h2>\n",
    "    <p>The most commercially valuable output of this model is not its accuracy — it is its <strong>mistakes</strong>.</p>\n",
    "    <p>A <strong>False Positive</strong> is a hexagon where:</p>\n",
    "    <ul>\n",
    "        <li>The model predicts <code>y=1</code> (this location <em>should</em> have a coffee shop)</li>\n",
    "        <li>The ground truth is <code>y=0</code> (no coffee shop currently exists here)</li>\n",
    "    </ul>\n",
    "    <p>These hexagons possess <em>all the learned features</em> of successful coffee shop locations —\n",
    "    high footfall, educated demographics, strong transit connectivity, synergy with gyms and offices —\n",
    "    but <strong>no one has opened a shop there yet</strong>.</p>\n",
    "    <p>In the language of network theory, these are <strong>Structural Holes</strong> (Burt, 1992) —\n",
    "    now identified not by heuristic scoring but by supervised machine learning.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SECTION 9: Extract False Positives as Site Recommendations\n# ============================================================\n\n# Retrain best model on full dataset for deployment predictions.\n# OOF evaluation was completed in Section 8; here we use the full-data\n# model to generate site recommendations (standard ML deployment practice).\nbest_model = grid_search.best_estimator_\nbest_model.fit(X, y)\n\ny_pred = best_model.predict(X)\ny_prob_best = best_model.predict_proba(X)[:, 1]\n\nh3_grid['predicted'] = y_pred\nh3_grid['actual'] = y.values\nh3_grid['predicted_prob'] = y_prob_best\n\n# Classify each hex by confusion matrix quadrant\ndef classify_outcome(row):\n    if row['predicted'] == 1 and row['actual'] == 0:\n        return 'False Positive (Recommendation)'\n    elif row['predicted'] == 1 and row['actual'] == 1:\n        return 'True Positive'\n    elif row['predicted'] == 0 and row['actual'] == 0:\n        return 'True Negative'\n    else:\n        return 'False Negative'\n\nh3_grid['outcome'] = h3_grid.apply(classify_outcome, axis=1)\n\nprint(\"Prediction Outcome Distribution:\")\nprint(h3_grid['outcome'].value_counts().to_string())\n\n# Extract and rank False Positives\nfp = h3_grid[h3_grid['outcome'] == 'False Positive (Recommendation)'].copy()\nfp_ranked = fp.nlargest(10, 'predicted_prob')\n\nprint(f\"\\n{'='*60}\")\nprint(f\"TOP 10 RECOMMENDED SITES (False Positives)\")\nprint(f\"{'='*60}\")\nfp_ranked[[\n    'h3_index', 'predicted_prob', 'population',\n    'level4_perc', 'age_16_to_34_perc',\n    'n_synergy', 'n_anchors', 'betweenness_centrality'\n]]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 9b: Demographic Profile of Recommended Sites\n",
    "# ============================================================\n",
    "# Compare FP sites vs Camden average\n",
    "profile_cols = ['population', 'level4_perc', 'age_16_to_34_perc',\n",
    "                'employed_total_perc', 'betweenness_centrality',\n",
    "                'n_synergy', 'n_anchors']\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Camden Average': h3_grid[profile_cols].mean(),\n",
    "    'Recommended Sites (FP)': fp[profile_cols].mean() if len(fp) > 0 else 0,\n",
    "    'Existing Shops (TP)': h3_grid[h3_grid['outcome'] == 'True Positive'][profile_cols].mean()\n",
    "})\n",
    "\n",
    "print(\"Demographic Profile Comparison:\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #7d3c98; border-bottom: 2px solid #7d3c98; padding-bottom: 10px;\">Section 10: Interactive 3D Recommendation Map</h2>\n",
    "    <p>We visualise the full confusion matrix spatially using <strong>Pydeck</strong>:</p>\n",
    "    <ul>\n",
    "        <li style=\"color: #27ae60;\"><strong>Green (extruded)</strong>: False Positives — our site recommendations</li>\n",
    "        <li style=\"color: #e74c3c;\"><strong>Red (flat)</strong>: True Positives — existing coffee shops</li>\n",
    "        <li style=\"color: #95a5a6;\"><strong>Grey (flat)</strong>: True Negatives — not suitable</li>\n",
    "        <li style=\"color: #f39c12;\"><strong>Orange (flat)</strong>: False Negatives — missed by model</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 10: Pydeck 3D Recommendation Map\n",
    "# ============================================================\n",
    "viz_df = h3_grid.to_crs(epsg=4326).copy()\n",
    "\n",
    "# Colour coding by confusion matrix quadrant\n",
    "color_map = {\n",
    "    'False Positive (Recommendation)': [39, 174, 96, 200],   # Green\n",
    "    'True Positive':                   [231, 76, 60, 160],   # Red\n",
    "    'True Negative':                   [149, 165, 166, 80],  # Grey\n",
    "    'False Negative':                  [243, 156, 18, 160],  # Orange\n",
    "}\n",
    "\n",
    "viz_df['color'] = viz_df['outcome'].map(color_map)\n",
    "\n",
    "# Elevation: extrude FP sites by confidence, keep others flat\n",
    "viz_df['elevation'] = viz_df.apply(\n",
    "    lambda r: r['predicted_prob'] * 500\n",
    "    if r['outcome'] == 'False Positive (Recommendation)' else 10,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "layer = pdk.Layer(\n",
    "    'H3HexagonLayer',\n",
    "    viz_df,\n",
    "    pickable=True,\n",
    "    stroked=True,\n",
    "    filled=True,\n",
    "    extruded=True,\n",
    "    get_hexagon='h3_index',\n",
    "    get_fill_color='color',\n",
    "    get_elevation='elevation',\n",
    "    elevation_scale=1,\n",
    ")\n",
    "\n",
    "view_state = pdk.ViewState(\n",
    "    latitude=51.54, longitude=-0.14,\n",
    "    zoom=12.5, pitch=50, bearing=-15\n",
    ")\n",
    "\n",
    "tooltip = {\n",
    "    'html': (\n",
    "        '<b>H3:</b> {h3_index}<br>'\n",
    "        '<b>Outcome:</b> {outcome}<br>'\n",
    "        '<b>P(coffee):</b> {predicted_prob}<br>'\n",
    "        '<b>Population:</b> {population}<br>'\n",
    "        '<b>Level 4%:</b> {level4_perc}<br>'\n",
    "        '<b>Synergy:</b> {n_synergy} | <b>Anchors:</b> {n_anchors}'\n",
    "    ),\n",
    "    'style': {\n",
    "        'backgroundColor': '#1a1a2e',\n",
    "        'color': 'white',\n",
    "        'fontSize': '12px'\n",
    "    }\n",
    "}\n",
    "\n",
    "deck = pdk.Deck(layers=[layer], initial_view_state=view_state, tooltip=tooltip)\n",
    "deck.to_html('data/outputs/camden_ml_recommendations.html')\n",
    "\n",
    "print(\"Interactive 3D map saved: data/outputs/camden_ml_recommendations.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 11: Export Final Results\n",
    "# ============================================================\n",
    "\n",
    "# 1. Full grid with predictions\n",
    "h3_grid.to_parquet('data/outputs/camden_ml_scored.parquet')\n",
    "\n",
    "# 2. Top 20 FP recommendations with lat/lon\n",
    "fp_top20 = h3_grid[\n",
    "    h3_grid['outcome'] == 'False Positive (Recommendation)'\n",
    "].nlargest(20, 'predicted_prob').copy()\n",
    "\n",
    "fp_export = fp_top20.to_crs(epsg=4326)\n",
    "fp_export['latitude'] = fp_export.geometry.centroid.y\n",
    "fp_export['longitude'] = fp_export.geometry.centroid.x\n",
    "\n",
    "export_cols = [\n",
    "    'h3_index', 'latitude', 'longitude', 'predicted_prob',\n",
    "    'population', 'level4_perc', 'age_16_to_34_perc',\n",
    "    'employed_total_perc', 'n_synergy', 'n_anchors',\n",
    "    'betweenness_centrality', 'closeness_centrality'\n",
    "]\n",
    "fp_export[export_cols].to_csv('data/outputs/fp_recommendations.csv', index=False)\n",
    "\n",
    "# 3. Model comparison summary\n",
    "results_df.to_csv('data/outputs/model_comparison.csv', index=False)\n",
    "\n",
    "print(\"Final outputs saved:\")\n",
    "print(\"  data/outputs/camden_ml_scored.parquet    (full grid + predictions)\")\n",
    "print(\"  data/outputs/fp_recommendations.csv       (top 20 site recommendations)\")\n",
    "print(\"  data/outputs/model_comparison.csv         (model AUC summary)\")\n",
    "print(\"  data/outputs/camden_ml_recommendations.html (interactive 3D map)\")\n",
    "print(\"  data/outputs/roc_curves.png               (ROC comparison chart)\")\n",
    "print(\"  data/outputs/confusion_matrix.png          (confusion matrix)\")\n",
    "print(\"  data/outputs/feature_importance.png        (feature importance chart)\")\n",
    "print(f\"\\nTotal False Positive recommendations: {len(fp)}\")\n",
    "print(\"Pipeline complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}