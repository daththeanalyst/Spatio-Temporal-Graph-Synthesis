{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 25px; background-color: #1a1a2e; border-radius: 12px; border: 2px solid #e94560;\">\n",
    "    <h1 style=\"color: #e94560; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Predictive Site Selection Model</h1>\n",
    "    <h2 style=\"color: #0f3460;\">Binary Classification on H3 Spatial Hexagons — Camden Specialty Coffee</h2>\n",
    "    <p style=\"color: #eee; font-size: 1.1em;\">\n",
    "        Predicting specialty coffee shop suitability from multi-modal geospatial features:<br>\n",
    "        <strong>LandScan</strong> population rasters, <strong>ONS Census 2021</strong> demographics, and <strong>NetworkX</strong> graph centrality.\n",
    "    </p>\n",
    "    <hr style=\"border-color: #e94560;\">\n",
    "    <div style=\"display: flex; justify-content: space-between; color: #aaa;\">\n",
    "        <span>London Borough of Camden</span>\n",
    "        <span>MSc Business Analytics | 2026</span>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px; padding: 15px; background-color: #eaf2f8; border-radius: 8px; border-left: 5px solid #2e86c1;\">\n",
    "    <h2 style=\"color: #1a5276;\">Pipeline Overview</h2>\n",
    "    <p>This notebook is <strong>self-contained</strong>. It re-derives all features from raw data sources so it can run independently of Notebooks 01–03. The pipeline:</p>\n",
    "    <ol>\n",
    "        <li><strong>Data Assembly</strong>: Load H3 grid, LandScan raster, OSM POIs, and 3 ONS Census CSVs.</li>\n",
    "        <li><strong>Feature Engineering</strong>: 12 features across 4 modalities (footfall, demographics, graph centrality, POI ecosystem).</li>\n",
    "        <li><strong>Target Definition</strong>: Binary — <code>has_coffee_shop</code> (1/0) per hexagon.</li>\n",
    "        <li><strong>Spatial Cross-Validation</strong>: H3 parent-cell block CV to prevent spatial leakage.</li>\n",
    "        <li><strong>Model Comparison</strong>: Logistic Regression vs. Random Forest vs. XGBoost.</li>\n",
    "        <li><strong>Hyperparameter Tuning</strong>: GridSearchCV with spatial folds.</li>\n",
    "        <li><strong>Business Insight</strong>: Extract False Positives as site recommendations.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 0: Environment & Imports\n",
    "# ============================================================\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import h3\n",
    "import rasterio\n",
    "import rasterstats\n",
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydeck as pdk\n",
    "from shapely.geometry import Polygon, Point\n",
    "\n",
    "# ML Stack\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score,\n",
    "    roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Directory scaffold\n",
    "for d in ['data/raw', 'data/processed', 'data/outputs']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"H3 version: {h3.__version__}\")\n",
    "print(f\"Environment ready. Random state: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #e94560; border-bottom: 2px solid #e94560; padding-bottom: 10px;\">Section 1: Data Preparation — The Feature Matrix</h2>\n",
    "    <p>We assemble a single flat <code>DataFrame</code> by joining three data modalities onto the H3 hexagonal grid.</p>\n",
    "    <table style=\"width:100%; border-collapse: collapse; margin-top: 10px; font-size: 0.95em;\">\n",
    "        <tr style=\"background: #16213e; color: white;\">\n",
    "            <th style=\"padding: 8px; text-align: left;\">Modality</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Source</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Features</th>\n",
    "        </tr>\n",
    "        <tr style=\"background: #f8f9fa;\"><td style=\"padding: 8px;\">Footfall</td><td>LandScan Raster (zonal stats)</td><td><code>population</code></td></tr>\n",
    "        <tr><td style=\"padding: 8px;\">Demographics</td><td>ONS Census 2021 (Digimap)</td><td><code>employed_total_perc</code>, <code>age_16_to_34_perc</code>, <code>level4_perc</code>, <code>retired_perc</code>, <code>no_qualifications_perc</code></td></tr>\n",
    "        <tr style=\"background: #f8f9fa;\"><td style=\"padding: 8px;\">Graph Centrality</td><td>NetworkX on H3 adjacency</td><td><code>degree_centrality</code>, <code>betweenness_centrality</code>, <code>closeness_centrality</code>, <code>clustering_coeff</code></td></tr>\n",
    "        <tr><td style=\"padding: 8px;\">POI Ecosystem</td><td>OSMnx spatial join</td><td><code>n_synergy</code>, <code>n_anchors</code> (competitors excluded — they <em>are</em> the target)</td></tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.1: Generate H3 Grid over Camden\n",
    "# ============================================================\n",
    "PLACE = \"London Borough of Camden\"\n",
    "RESOLUTION = 9  # ~174m edge length — walking scale\n",
    "\n",
    "# Fetch Camden boundary\n",
    "boundary = ox.geocode_to_gdf(PLACE)\n",
    "boundary_wgs84 = boundary.to_crs(epsg=4326)\n",
    "poly = boundary_wgs84.geometry.iloc[0]\n",
    "\n",
    "# H3 v4: polygon_to_cells\n",
    "outer_coords = [(lat, lng) for lng, lat in poly.exterior.coords]\n",
    "holes = [[(lat, lng) for lng, lat in ring.coords] for ring in poly.interiors]\n",
    "h3_poly = h3.LatLngPoly(outer_coords, *holes)\n",
    "hex_ids = h3.polygon_to_cells(h3_poly, RESOLUTION)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "def h3_to_shapely(cell_id):\n",
    "    coords = h3.cell_to_boundary(cell_id)\n",
    "    return Polygon([(lng, lat) for lat, lng in coords])\n",
    "\n",
    "hex_polygons = [h3_to_shapely(h) for h in hex_ids]\n",
    "h3_grid = gpd.GeoDataFrame(\n",
    "    {'h3_index': list(hex_ids)},\n",
    "    geometry=hex_polygons,\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "\n",
    "print(f\"H3 Grid: {len(h3_grid)} hexagons at Resolution {RESOLUTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.2: Footfall — LandScan Raster Enrichment\n",
    "# ============================================================\n",
    "raster_path = \"landscan-mosaic-unitedkingdom-v1.tif\"\n",
    "\n",
    "with rasterio.open(raster_path) as src:\n",
    "    print(f\"Raster CRS: {src.crs} | Resolution: {src.res}\")\n",
    "\n",
    "# Zonal stats: sum population pixels under each hex\n",
    "stats = rasterstats.zonal_stats(h3_grid, raster_path, stats=['sum'], nodata=-999)\n",
    "h3_grid['population'] = [s['sum'] if s['sum'] is not None else 0 for s in stats]\n",
    "\n",
    "print(f\"Total LandScan population across Camden hexes: {h3_grid['population'].sum():,.0f}\")\n",
    "print(f\"Max hex population: {h3_grid['population'].max():,.0f}\")\n",
    "print(f\"Hexes with zero population: {(h3_grid['population'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.3: POI Ecosystem — OSMnx Fetch & Categorisation\n",
    "# ============================================================\n",
    "tags = {\n",
    "    'amenity': ['cafe', 'coffee_shop', 'gym', 'university', 'office',\n",
    "                'library', 'leisure_centre'],\n",
    "    'leisure': ['fitness_centre', 'sports_centre'],\n",
    "    'shop': ['bakery', 'supermarket'],\n",
    "    'public_transport': ['station']\n",
    "}\n",
    "\n",
    "pois_raw = ox.features_from_place(PLACE, tags)\n",
    "pois_raw['geometry'] = pois_raw.to_crs(epsg=27700).centroid\n",
    "pois_raw = pois_raw.set_crs(epsg=27700)\n",
    "pois = pois_raw[pois_raw.geometry.type == 'Point'].copy()\n",
    "pois = pois.to_crs(epsg=4326)  # match h3_grid CRS for sjoin\n",
    "\n",
    "# Categorise by business role\n",
    "def categorize(row):\n",
    "    amenity = row.get('amenity', '')\n",
    "    leisure = row.get('leisure', '')\n",
    "    transport = row.get('public_transport', '')\n",
    "    if amenity in ('cafe', 'coffee_shop'):\n",
    "        return 'Competitor'\n",
    "    if amenity in ('gym', 'university', 'office', 'library', 'leisure_centre') \\\n",
    "       or leisure in ('fitness_centre', 'sports_centre'):\n",
    "        return 'Synergy'\n",
    "    if transport == 'station':\n",
    "        return 'Anchor'\n",
    "    return 'Other'\n",
    "\n",
    "pois['role'] = pois.apply(categorize, axis=1)\n",
    "\n",
    "print(f\"POIs fetched: {len(pois)}\")\n",
    "print(pois['role'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.4: Spatial Join — POI Counts per Hexagon\n",
    "# ============================================================\n",
    "pois_in_hex = gpd.sjoin(pois, h3_grid[['h3_index', 'geometry']], how='inner', predicate='within')\n",
    "\n",
    "# Pivot by role\n",
    "role_counts = pois_in_hex.groupby(['h3_index', 'role']).size().unstack(fill_value=0)\n",
    "\n",
    "# Ensure all role columns exist\n",
    "for col in ['Competitor', 'Synergy', 'Anchor', 'Other']:\n",
    "    if col not in role_counts.columns:\n",
    "        role_counts[col] = 0\n",
    "\n",
    "role_counts = role_counts.rename(columns={\n",
    "    'Competitor': 'n_competitors',\n",
    "    'Synergy': 'n_synergy',\n",
    "    'Anchor': 'n_anchors',\n",
    "    'Other': 'n_other'\n",
    "})\n",
    "\n",
    "h3_grid = h3_grid.merge(role_counts, on='h3_index', how='left')\n",
    "for col in ['n_competitors', 'n_synergy', 'n_anchors', 'n_other']:\n",
    "    h3_grid[col] = h3_grid[col].fillna(0).astype(int)\n",
    "\n",
    "print(f\"Competitors: {h3_grid['n_competitors'].sum()} | Synergy: {h3_grid['n_synergy'].sum()} | Anchors: {h3_grid['n_anchors'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.5: Demographics — ONS Census Enrichment\n",
    "# ============================================================\n",
    "# Load all 3 census CSVs\n",
    "econ_df = pd.read_csv('ons-economic-ew-2021_6304504/ons-economic-ew-2021.csv')\n",
    "age_df  = pd.read_csv('ons-age-ew-2021_6304503/ons-age-ew-2021.csv')\n",
    "qual_df = pd.read_csv('ons-qualifications-ew-2021_6304505/ons-qualifications-ew-2021.csv')\n",
    "\n",
    "# Merge on geog_code\n",
    "census = econ_df[['geog_code', 'centroid_x', 'centroid_y', 'denom_total',\n",
    "                   'employed_total_perc', 'retired_perc', 'unemployed_perc']].copy()\n",
    "census = census.merge(\n",
    "    age_df[['geog_code', 'age_16_to_34_perc', 'age_65_plus_perc']], on='geog_code', how='left'\n",
    ")\n",
    "census = census.merge(\n",
    "    qual_df[['geog_code', 'level4_perc', 'no_qualifications_perc']], on='geog_code', how='left'\n",
    ")\n",
    "\n",
    "# Convert to GeoDataFrame (centroids are already BNG EPSG:27700)\n",
    "geometry = [Point(xy) for xy in zip(census['centroid_x'], census['centroid_y'])]\n",
    "census_gdf = gpd.GeoDataFrame(census, geometry=geometry, crs='EPSG:27700')\n",
    "census_gdf = census_gdf.to_crs(epsg=4326)  # match h3_grid\n",
    "\n",
    "# Spatial join: nearest census OA centroid -> hex\n",
    "census_in_hex = gpd.sjoin_nearest(\n",
    "    census_gdf, h3_grid[['h3_index', 'geometry']],\n",
    "    how='left', max_distance=0.005\n",
    ")\n",
    "\n",
    "# Population-weighted mean of demographics per hex\n",
    "demo_cols = ['employed_total_perc', 'age_16_to_34_perc', 'level4_perc',\n",
    "             'retired_perc', 'no_qualifications_perc']\n",
    "\n",
    "def weighted_mean(group, col):\n",
    "    w = group['denom_total']\n",
    "    return (group[col] * w).sum() / w.sum() if w.sum() > 0 else 0\n",
    "\n",
    "hex_demos = []\n",
    "for h3_id, group in census_in_hex.groupby('h3_index'):\n",
    "    row = {'h3_index': h3_id}\n",
    "    for col in demo_cols:\n",
    "        row[col] = weighted_mean(group, col)\n",
    "    hex_demos.append(row)\n",
    "\n",
    "demo_df = pd.DataFrame(hex_demos)\n",
    "h3_grid = h3_grid.merge(demo_df, on='h3_index', how='left')\n",
    "for col in demo_cols:\n",
    "    h3_grid[col] = h3_grid[col].fillna(0)\n",
    "\n",
    "print(f\"Census demographics joined. Coverage: {len(demo_df)}/{len(h3_grid)} hexes\")\n",
    "print(f\"  Mean Level 4%: {h3_grid['level4_perc'].mean():.1f}%\")\n",
    "print(f\"  Mean Age 16-34%: {h3_grid['age_16_to_34_perc'].mean():.1f}%\")\n",
    "print(f\"  Mean Employment%: {h3_grid['employed_total_perc'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #0f3460; border-bottom: 2px solid #0f3460; padding-bottom: 10px;\">Section 1.6: Graph Centrality Features</h2>\n",
    "    <p>We construct the H3 adjacency graph and compute four centrality metrics per hexagon:</p>\n",
    "    <ul>\n",
    "        <li><strong>Degree Centrality</strong>: $C_D(v) = \\frac{\\deg(v)}{N-1}$ — connectivity (number of neighbours).</li>\n",
    "        <li><strong>Betweenness Centrality</strong>: $C_B(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}$ — bridging position on shortest paths.</li>\n",
    "        <li><strong>Closeness Centrality</strong>: $C_C(v) = \\frac{N-1}{\\sum_{u} d(v,u)}$ — accessibility to all other hexes.</li>\n",
    "        <li><strong>Clustering Coefficient</strong>: $C_{clust}(v) = \\frac{2 T(v)}{\\deg(v)(\\deg(v)-1)}$ — neighbourhood cohesion.</li>\n",
    "    </ul>\n",
    "    <p>These capture a hexagon's <em>structural role</em> in the urban network — independent of its content.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.6: Graph Centrality Features\n",
    "# ============================================================\n",
    "G = nx.Graph()\n",
    "hex_set = set(h3_grid['h3_index'])\n",
    "\n",
    "for h3_id in hex_set:\n",
    "    G.add_node(h3_id)\n",
    "\n",
    "for h3_id in hex_set:\n",
    "    for nb in h3.grid_disk(h3_id, 1):\n",
    "        if nb != h3_id and nb in hex_set:\n",
    "            G.add_edge(h3_id, nb)\n",
    "\n",
    "# Compute centrality metrics\n",
    "h3_grid['degree_centrality'] = h3_grid['h3_index'].map(nx.degree_centrality(G))\n",
    "h3_grid['betweenness_centrality'] = h3_grid['h3_index'].map(nx.betweenness_centrality(G))\n",
    "h3_grid['closeness_centrality'] = h3_grid['h3_index'].map(nx.closeness_centrality(G))\n",
    "h3_grid['clustering_coeff'] = h3_grid['h3_index'].map(nx.clustering(G))\n",
    "\n",
    "print(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "print(f\"Mean degree centrality: {h3_grid['degree_centrality'].mean():.4f}\")\n",
    "print(f\"Mean betweenness centrality: {h3_grid['betweenness_centrality'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<div style=\"margin-top: 30px;\">\n    <h2 style=\"color: #e94560; border-bottom: 2px solid #e94560; padding-bottom: 10px;\">Section 2: Target Variable — Binary Coffee Shop Presence</h2>\n    <p>We define the prediction target as:</p>\n    <p style=\"text-align: center; font-size: 1.2em;\">$$y_H = \\begin{cases} 1 & \\text{if hexagon } H \\text{ contains } \\geq 1 \\text{ cafe or coffee\\_shop} \\\\ 0 & \\text{otherwise} \\end{cases}$$</p>\n    <p><strong>Class imbalance is expected</strong>: most hexagons will not contain a coffee shop. We handle this with <code>class_weight='balanced'</code> (LR, RF) and <code>scale_pos_weight</code> (XGBoost).</p>\n\n    <div style=\"background: #eaf2f8; padding: 12px; border-radius: 5px; border-left: 4px solid #2e86c1; margin-top: 10px;\">\n        <strong>Success Metrics &amp; Constraints</strong>\n        <ul style=\"margin: 8px 0;\">\n            <li><strong>Primary metric</strong>: ROC-AUC &mdash; threshold-invariant measure of discrimination. Chosen over accuracy because class imbalance makes accuracy misleading (a naive all-zero classifier achieves ~80% accuracy).</li>\n            <li><strong>Secondary</strong>: Precision at top-20 &mdash; are the top-ranked recommendations credible to an investor?</li>\n            <li><strong>Tertiary</strong>: Recall &mdash; do we miss viable locations? Low recall means leaving revenue on the table.</li>\n        </ul>\n        <strong>Scope constraints:</strong>\n        <ul style=\"margin: 8px 0;\">\n            <li>Single borough (Camden) &mdash; model may not generalise to other London boroughs without retraining.</li>\n            <li>Static temporal snapshot: ONS Census 2021, OSM data from the notebook run date. No longitudinal dynamics.</li>\n            <li>Binary target (presence/absence) &mdash; no ground-truth revenue or profitability data available.</li>\n            <li>No commercial rent, planning permission, or lease availability data &mdash; the model identifies <em>demand-side</em> opportunity only.</li>\n        </ul>\n    </div>\n\n    <div style=\"background: #fff3cd; padding: 12px; border-radius: 5px; border-left: 4px solid #ffc107; margin-top: 10px;\">\n        <strong>Leakage Warning:</strong> The <code>n_competitors</code> column counts cafes/coffee shops per hex &mdash; this directly encodes the target.\n        It <strong>must be excluded</strong> from the feature matrix <code>X</code>. We retain <code>n_synergy</code> and <code>n_anchors</code>\n        as legitimate predictors (synergy nodes attract coffee shops, not vice versa).\n    </div>\n</div>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 2: Target Definition & Feature Matrix\n",
    "# ============================================================\n",
    "\n",
    "# Binary target\n",
    "h3_grid['has_coffee_shop'] = (h3_grid['n_competitors'] >= 1).astype(int)\n",
    "\n",
    "print(\"Target distribution:\")\n",
    "print(h3_grid['has_coffee_shop'].value_counts())\n",
    "print(f\"Positive class rate: {h3_grid['has_coffee_shop'].mean():.1%}\")\n",
    "\n",
    "# === Feature set (LEAKAGE GUARD: n_competitors excluded) ===\n",
    "FEATURE_COLS = [\n",
    "    # Footfall\n",
    "    'population',\n",
    "    # Demographics\n",
    "    'employed_total_perc',\n",
    "    'age_16_to_34_perc',\n",
    "    'level4_perc',\n",
    "    'retired_perc',\n",
    "    'no_qualifications_perc',\n",
    "    # Graph Centrality\n",
    "    'degree_centrality',\n",
    "    'betweenness_centrality',\n",
    "    'closeness_centrality',\n",
    "    'clustering_coeff',\n",
    "    # POI Ecosystem (synergy + anchors ONLY)\n",
    "    'n_synergy',\n",
    "    'n_anchors',\n",
    "]\n",
    "\n",
    "TARGET_COL = 'has_coffee_shop'\n",
    "\n",
    "X = h3_grid[FEATURE_COLS].copy()\n",
    "y = h3_grid[TARGET_COL].copy()\n",
    "\n",
    "# Leakage assertion\n",
    "assert 'n_competitors' not in X.columns, \"LEAKAGE: n_competitors found in feature set!\"\n",
    "\n",
    "print(f\"\\nFeature matrix X: {X.shape}\")\n",
    "print(f\"Features ({len(FEATURE_COLS)}): {FEATURE_COLS}\")\n",
    "print(f\"\\nMissing values per feature:\")\n",
    "print(X.isnull().sum().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #0f3460; border-bottom: 2px solid #0f3460; padding-bottom: 10px;\">Section 3: Exploratory Data Analysis & Leakage Audit</h2>\n",
    "    <p>Before modelling, we verify:</p>\n",
    "    <ol>\n",
    "        <li><strong>Feature distributions</strong>: Zero variance, extreme skew, or missing values?</li>\n",
    "        <li><strong>Multicollinearity</strong>: Variance Inflation Factors (VIF) — flag features with VIF &gt; 10.</li>\n",
    "        <li><strong>Spatial autocorrelation</strong>: Visual confirmation that the target is spatially clustered (motivating Spatial CV).</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# SECTION 3-pre: Missingness Audit & Class Balance\n# ============================================================\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# --- Left: Missingness summary ---\nmissing = X.isnull().sum()\nmissing_pct = (X.isnull().sum() / len(X) * 100)\nmiss_df = pd.DataFrame({'Count': missing, 'Percent': missing_pct}).sort_values('Count', ascending=False)\n\naxes[0].barh(miss_df.index, miss_df['Percent'], color='#e74c3c')\naxes[0].set_xlabel('Missing (%)')\naxes[0].set_title('Feature Missingness', fontsize=12, fontweight='bold')\naxes[0].axvline(x=5, color='gray', linestyle='--', alpha=0.5, label='5% threshold')\naxes[0].legend()\n\n# --- Right: Class balance ---\nclass_counts = y.value_counts().sort_index()\nbars = axes[1].bar(\n    ['No Coffee (0)', 'Has Coffee (1)'],\n    class_counts.values,\n    color=['#3498db', '#e74c3c']\n)\nfor bar, val in zip(bars, class_counts.values):\n    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n                 str(val), ha='center', fontweight='bold')\naxes[1].set_title('Target Class Distribution', fontsize=12, fontweight='bold')\naxes[1].set_ylabel('Number of Hexagons')\n\nimbalance = class_counts[0] / class_counts[1]\naxes[1].annotate(f'Imbalance ratio: {imbalance:.1f}:1',\n                 xy=(0.5, 0.85), xycoords='axes fraction',\n                 ha='center', fontsize=11, fontweight='bold',\n                 bbox=dict(boxstyle='round', facecolor='#fff3cd', edgecolor='#ffc107'))\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Missingness summary: {(missing > 0).sum()} features have missing values\")\nprint(f\"Class balance: {class_counts[0]} negative, {class_counts[1]} positive \"\n      f\"(ratio {imbalance:.1f}:1)\")\nprint(f\"  -> A naive all-zero classifier achieves {class_counts[0]/len(y)*100:.1f}% accuracy\")\nprint(f\"  -> This motivates ROC-AUC as the primary metric (threshold-invariant)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 3a: Correlation Heatmap\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "corr = X.join(y).corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(\n",
    "    corr, mask=mask, annot=True, fmt='.2f',\n",
    "    cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "    square=True, linewidths=0.5, ax=ax\n",
    ")\n",
    "ax.set_title('Feature Correlation Matrix (incl. Target)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/outputs/eda_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Flag high correlations with target\n",
    "target_corr = corr[TARGET_COL].drop(TARGET_COL).sort_values(ascending=False)\n",
    "print(\"Feature correlations with target (has_coffee_shop):\")\n",
    "print(target_corr.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 3b: Feature Distributions by Target Class\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "key_features = ['population', 'level4_perc', 'age_16_to_34_perc',\n",
    "                'betweenness_centrality', 'n_synergy', 'n_anchors']\n",
    "\n",
    "for ax, feat in zip(axes.ravel(), key_features):\n",
    "    plot_df = X[[feat]].copy()\n",
    "    plot_df['target'] = y.values\n",
    "    sns.boxplot(data=plot_df, x='target', y=feat, ax=ax,\n",
    "                palette={0: '#3498db', 1: '#e74c3c'})\n",
    "    ax.set_title(feat, fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('has_coffee_shop')\n",
    "\n",
    "plt.suptitle('Feature Distributions by Target Class', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/outputs/eda_boxplots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 3c: Variance Inflation Factor (Multicollinearity)\n",
    "# ============================================================\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# VIF requires no NaN and no constant columns\n",
    "X_vif = X.fillna(0).copy()\n",
    "# Remove zero-variance columns for VIF calculation\n",
    "X_vif = X_vif.loc[:, X_vif.std() > 0]\n",
    "\n",
    "vif_data = pd.DataFrame({\n",
    "    'Feature': X_vif.columns,\n",
    "    'VIF': [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "}).sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"Variance Inflation Factors:\")\n",
    "print(vif_data.to_string(index=False))\n",
    "print(f\"\\nFeatures with VIF > 10 (potential multicollinearity): \"\n",
    "      f\"{list(vif_data[vif_data['VIF'] > 10]['Feature'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<div style=\"margin-top: 20px; padding: 15px; background-color: #fdf2e9; border-radius: 8px; border-left: 5px solid #d35400;\">\n    <h3 style=\"color: #d35400;\">Leakage Audit</h3>\n    <p>Before proceeding to modelling, we formally document potential leakage vectors and our mitigations:</p>\n    <table style=\"width:100%; border-collapse: collapse; margin: 10px 0; font-size: 0.9em;\">\n        <tr style=\"background: #d35400; color: white;\">\n            <th style=\"padding: 8px;\">Risk</th>\n            <th style=\"padding: 8px;\">Feature(s)</th>\n            <th style=\"padding: 8px;\">Assessment</th>\n            <th style=\"padding: 8px;\">Mitigation</th>\n        </tr>\n        <tr>\n            <td style=\"padding: 8px;\"><strong>Direct target encoding</strong></td>\n            <td><code>n_competitors</code></td>\n            <td style=\"color: #c0392b;\">HIGH &mdash; counts cafes/coffee shops, which <em>are</em> the target</td>\n            <td>Excluded from <code>X</code>. Enforced by runtime assertion.</td>\n        </tr>\n        <tr style=\"background: #fef9f4;\">\n            <td style=\"padding: 8px;\"><strong>Reverse causation</strong></td>\n            <td><code>n_synergy</code>, <code>n_anchors</code></td>\n            <td style=\"color: #e67e22;\">MODERATE &mdash; do coffee shops attract gyms, or do gyms attract coffee shops?</td>\n            <td>Retained. Causal direction is predominantly synergy &rarr; coffee (gyms/offices pre-date most cafes). Acknowledged as a limitation in the Model Card.</td>\n        </tr>\n        <tr>\n            <td style=\"padding: 8px;\"><strong>Temporal leakage</strong></td>\n            <td>All OSM features</td>\n            <td style=\"color: #e67e22;\">MODERATE &mdash; OSM snapshot is contemporaneous with target; no temporal split</td>\n            <td>Accepted. This is a cross-sectional study. Acknowledged in scope constraints.</td>\n        </tr>\n        <tr style=\"background: #fef9f4;\">\n            <td style=\"padding: 8px;\"><strong>Spatial leakage</strong></td>\n            <td>All features (via autocorrelation)</td>\n            <td style=\"color: #c0392b;\">HIGH &mdash; adjacent hexes share similar feature values (Tobler's Law)</td>\n            <td>Controlled by Spatial Block CV (Section 4). Adjacent hexes always in same fold.</td>\n        </tr>\n    </table>\n</div>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #c0392b; border-bottom: 2px solid #c0392b; padding-bottom: 10px;\">Section 4: Spatial Cross-Validation Strategy</h2>\n",
    "    <p><strong>Problem:</strong> Standard <code>KFold</code> randomly assigns hexagons to train/test folds.\n",
    "    Adjacent hexes share similar features due to spatial autocorrelation\n",
    "    (<em>Tobler's First Law of Geography</em>: \"near things are more related than distant things\").\n",
    "    Random splits leak spatial signal, inflating performance metrics by 5–15%.</p>\n",
    "    \n",
    "    <p><strong>Solution:</strong> <strong>Spatial Block Cross-Validation</strong> using the H3 hierarchy.\n",
    "    We group Resolution-9 hexagons by their Resolution-5 parent cell (∼10 km² blocks).\n",
    "    All hexes sharing a parent are assigned to the same fold, preserving spatial integrity.</p>\n",
    "    \n",
    "    <p style=\"text-align: center; font-size: 1.1em;\">$$\\text{fold}(H) = \\text{h3\\_cell\\_to\\_parent}(H, \\text{res}=5) \\bmod k$$</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 4: Spatial Block Cross-Validation (H3 Parent-Cell)\n",
    "# ============================================================\n",
    "\n",
    "class SpatialKFold:\n",
    "    \"\"\"Spatial block cross-validation using H3 parent-cell partitioning.\n",
    "\n",
    "    Groups H3 Res-9 hexagons by their Res-5 parent cell, then assigns\n",
    "    each spatial block to a fold. Ensures geographically proximate\n",
    "    hexagons never appear in both train and test simultaneously.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, parent_resolution=5):\n",
    "        self.n_splits = n_splits\n",
    "        self.parent_resolution = parent_resolution\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        parents = groups.apply(\n",
    "            lambda h: h3.cell_to_parent(h, self.parent_resolution)\n",
    "        )\n",
    "        unique_parents = parents.unique()\n",
    "        parent_to_fold = {\n",
    "            p: i % self.n_splits\n",
    "            for i, p in enumerate(unique_parents)\n",
    "        }\n",
    "        fold_assignments = parents.map(parent_to_fold)\n",
    "\n",
    "        for fold_idx in range(self.n_splits):\n",
    "            test_mask = fold_assignments == fold_idx\n",
    "            train_idx = np.where(~test_mask)[0]\n",
    "            test_idx = np.where(test_mask)[0]\n",
    "            yield train_idx, test_idx\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "\n",
    "spatial_cv = SpatialKFold(n_splits=5, parent_resolution=5)\n",
    "\n",
    "# Verify fold sizes\n",
    "print(\"Spatial CV Fold Sizes:\")\n",
    "for i, (train_idx, test_idx) in enumerate(\n",
    "    spatial_cv.split(X, y, groups=h3_grid['h3_index'])\n",
    "):\n",
    "    print(f\"  Fold {i+1}: train={len(train_idx)}, test={len(test_idx)}, \"\n",
    "          f\"test_positive_rate={y.iloc[test_idx].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #117a65; border-bottom: 2px solid #117a65; padding-bottom: 10px;\">Section 5: Baseline Model — Logistic Regression</h2>\n",
    "    <p>Logistic Regression serves as our <strong>interpretability baseline</strong>. Its linear decision boundary provides\n",
    "    directly readable coefficients — the sign and magnitude of each feature's effect on coffee shop presence.</p>\n",
    "    <p>We apply <code>StandardScaler</code> (required for LR convergence) and <code>class_weight='balanced'</code>\n",
    "    to handle the expected class imbalance.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 5: Baseline — Logistic Regression\n",
    "# ============================================================\n",
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(\n",
    "        class_weight='balanced', max_iter=1000, random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "lr_scores = cross_val_score(\n",
    "    lr_pipe, X, y,\n",
    "    cv=spatial_cv.split(X, y, groups=h3_grid['h3_index']),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f\"Logistic Regression (Spatial CV):\")\n",
    "print(f\"  ROC-AUC: {lr_scores.mean():.3f} \\u00b1 {lr_scores.std():.3f}\")\n",
    "print(f\"  Per-fold: {[f'{s:.3f}' for s in lr_scores]}\")\n",
    "\n",
    "# Fit on full data for coefficient inspection\n",
    "lr_pipe.fit(X, y)\n",
    "lr_coefs = pd.Series(\n",
    "    lr_pipe.named_steps['model'].coef_[0],\n",
    "    index=FEATURE_COLS\n",
    ").sort_values()\n",
    "\n",
    "print(f\"\\nLogistic Regression Coefficients (standardised):\")\n",
    "print(lr_coefs.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #884ea0; border-bottom: 2px solid #884ea0; padding-bottom: 10px;\">Section 6: Model Comparison — Random Forest & XGBoost</h2>\n",
    "    <p>We compare three models spanning the complexity spectrum:</p>\n",
    "    <table style=\"width:100%; border-collapse: collapse; margin-top: 10px;\">\n",
    "        <tr style=\"background: #16213e; color: white;\">\n",
    "            <th style=\"padding: 8px;\">Model</th><th>Type</th><th>Class Imbalance Handling</th><th>Rationale</th>\n",
    "        </tr>\n",
    "        <tr><td style=\"padding: 8px;\">Logistic Regression</td><td>Linear</td><td><code>class_weight='balanced'</code></td><td>Interpretable baseline</td></tr>\n",
    "        <tr style=\"background: #f8f9fa;\"><td style=\"padding: 8px;\">Random Forest</td><td>Bagged ensemble</td><td><code>class_weight='balanced'</code></td><td>Non-linear, robust to outliers</td></tr>\n",
    "        <tr><td style=\"padding: 8px;\">XGBoost</td><td>Boosted ensemble</td><td><code>scale_pos_weight</code></td><td>State-of-the-art tabular performance</td></tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SECTION 6: Model Comparison — LR vs RF vs XGBoost\n# ============================================================\nimbalance_ratio = (y == 0).sum() / (y == 1).sum()\n\nmodels = {\n    'Logistic Regression': Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', LogisticRegression(\n            class_weight='balanced', max_iter=1000, random_state=RANDOM_STATE\n        ))\n    ]),\n    'Random Forest': RandomForestClassifier(\n        n_estimators=200, class_weight='balanced',\n        random_state=RANDOM_STATE, n_jobs=-1\n    ),\n    'XGBoost': xgb.XGBClassifier(\n        n_estimators=200,\n        scale_pos_weight=imbalance_ratio,\n        eval_metric='logloss',\n        random_state=RANDOM_STATE\n    )\n}\n\nresults = {}\nfor name, model in models.items():\n    scores = cross_val_score(\n        model, X, y,\n        cv=spatial_cv.split(X, y, groups=h3_grid['h3_index']),\n        scoring='roc_auc'\n    )\n    results[name] = {\n        'mean_auc': scores.mean(),\n        'std_auc': scores.std(),\n        'scores': scores\n    }\n    print(f\"{name:25s} ROC-AUC = {scores.mean():.3f} \\u00b1 {scores.std():.3f}\")\n\n# Summary table\nresults_df = pd.DataFrame({\n    'Model': results.keys(),\n    'Mean AUC': [r['mean_auc'] for r in results.values()],\n    'Std AUC': [r['std_auc'] for r in results.values()]\n}).sort_values('Mean AUC', ascending=False)\n\nprint(f\"\\nModel Comparison Summary:\")\nresults_df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #d35400; border-bottom: 2px solid #d35400; padding-bottom: 10px;\">Section 7: Hyperparameter Tuning</h2>\n",
    "    <p>We tune the best-performing model using <code>GridSearchCV</code> with our spatial CV splitter.\n",
    "    The search space covers tree depth, learning rate, subsampling, and column sampling —\n",
    "    the most impactful XGBoost hyperparameters for tabular classification.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SECTION 7: Hyperparameter Tuning (XGBoost)\n# ============================================================\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.7, 0.9],\n    'colsample_bytree': [0.7, 1.0]\n}\n\nxgb_base = xgb.XGBClassifier(\n    scale_pos_weight=imbalance_ratio,\n    eval_metric='logloss',\n    random_state=RANDOM_STATE\n)\n\n# Pre-compute spatial CV splits for GridSearchCV\ncv_splits = list(spatial_cv.split(X, y, groups=h3_grid['h3_index']))\n\ngrid_search = GridSearchCV(\n    xgb_base, param_grid,\n    cv=cv_splits,\n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1,\n    refit=True\n)\n\ngrid_search.fit(X, y)\n\nprint(f\"\\nBest Parameters: {grid_search.best_params_}\")\nprint(f\"Best ROC-AUC (Spatial CV): {grid_search.best_score_:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #1a5276; border-bottom: 2px solid #1a5276; padding-bottom: 10px;\">Section 8: Evaluation Suite</h2>\n",
    "    <p>We present the full evaluation battery: ROC curves, confusion matrix, feature importance, and classification report.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SECTION 8a: ROC Curves — Out-of-Fold Predictions (Spatial CV)\n# ============================================================\nfrom sklearn.model_selection import cross_val_predict\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Materialise spatial CV splits once (reuse for all models)\ncv_splits = list(spatial_cv.split(X, y, groups=h3_grid['h3_index']))\n\nfor name, model in models.items():\n    # Out-of-fold predictions: each sample predicted only when in the test fold\n    y_prob_oof = cross_val_predict(\n        model, X, y, cv=cv_splits, method='predict_proba'\n    )[:, 1]\n    fpr, tpr, _ = roc_curve(y, y_prob_oof)\n    auc_val = roc_auc_score(y, y_prob_oof)\n    ax.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC={auc_val:.3f})')\n\n# Tuned XGBoost: build a fresh estimator with best params for OOF evaluation\nbest_params = grid_search.best_params_\ntuned_xgb = xgb.XGBClassifier(\n    **best_params,\n    scale_pos_weight=imbalance_ratio,\n    eval_metric='logloss',\n    random_state=RANDOM_STATE\n)\ny_prob_oof_best = cross_val_predict(\n    tuned_xgb, X, y, cv=cv_splits, method='predict_proba'\n)[:, 1]\nfpr_b, tpr_b, _ = roc_curve(y, y_prob_oof_best)\nauc_best = roc_auc_score(y, y_prob_oof_best)\nax.plot(fpr_b, tpr_b, linewidth=2.5, linestyle='--',\n        label=f'XGBoost Tuned (AUC={auc_best:.3f})')\n\nax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random Baseline')\nax.set_xlabel('False Positive Rate', fontsize=12)\nax.set_ylabel('True Positive Rate', fontsize=12)\nax.set_title('ROC Curves: Spatial CV Out-of-Fold Predictions', fontsize=14, fontweight='bold')\nax.legend(loc='lower right', fontsize=10)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('data/outputs/roc_curves.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SECTION 8b: Confusion Matrix — Out-of-Fold (Spatial CV)\n# ============================================================\n# Use OOF predictions from the tuned XGBoost for honest evaluation\ny_pred_oof = cross_val_predict(tuned_xgb, X, y, cv=cv_splits)\n\nfig, ax = plt.subplots(figsize=(6, 5))\ncm = confusion_matrix(y, y_pred_oof)\ndisp = ConfusionMatrixDisplay(cm, display_labels=['No Coffee (0)', 'Has Coffee (1)'])\ndisp.plot(cmap='Blues', ax=ax, values_format='d')\nax.set_title('Confusion Matrix \\u2014 Spatial CV Out-of-Fold', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig('data/outputs/confusion_matrix.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nClassification Report (Out-of-Fold, Spatial CV):\")\nprint(classification_report(y, y_pred_oof, target_names=['No Coffee', 'Has Coffee']))"
  },
  {
   "cell_type": "markdown",
   "source": "<div style=\"margin-top: 30px;\">\n    <h2 style=\"color: #8e44ad; border-bottom: 2px solid #8e44ad; padding-bottom: 10px;\">Section 8d: Calibration &amp; Error Analysis</h2>\n    <p>Beyond discrimination (ROC-AUC), we assess <strong>calibration</strong> (are predicted probabilities trustworthy?)\n    and <strong>failure modes</strong> (where and why does the model err?).</p>\n</div>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# SECTION 8d: Calibration Curve\n# ============================================================\nfrom sklearn.calibration import calibration_curve\n\nfig, ax = plt.subplots(figsize=(7, 6))\n\n# Calibration curve: do predicted probabilities match observed frequencies?\nfraction_pos, mean_predicted = calibration_curve(y, y_prob_oof_best, n_bins=8, strategy='uniform')\n\nax.plot(mean_predicted, fraction_pos, 's-', color='#e94560', linewidth=2, label='Tuned XGBoost')\nax.plot([0, 1], [0, 1], 'k--', alpha=0.4, label='Perfectly Calibrated')\nax.set_xlabel('Mean Predicted Probability', fontsize=12)\nax.set_ylabel('Observed Fraction of Positives', fontsize=12)\nax.set_title('Calibration Curve — OOF Predictions', fontsize=14, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Interpretation\nprint(\"Calibration check:\")\nfor mp, fp_val in zip(mean_predicted, fraction_pos):\n    print(f\"  Predicted ~{mp:.2f} -> Observed {fp_val:.2f}\")\nprint(\"\\nIf the model says 60% probability, do ~60% of those hexes actually have coffee?\")\nprint(\"Points above the diagonal = under-confident; below = over-confident.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# SECTION 8e: Failure Mode Analysis — FP vs TP vs FN Feature Profiles\n# ============================================================\n\n# Classify OOF predictions by confusion matrix quadrant\noof_outcome = pd.Series('TN', index=y.index)\noof_outcome[(y_pred_oof == 1) & (y == 0)] = 'FP'\noof_outcome[(y_pred_oof == 1) & (y == 1)] = 'TP'\noof_outcome[(y_pred_oof == 0) & (y == 1)] = 'FN'\n\n# Compare feature profiles across outcome groups\nanalysis_features = ['population', 'level4_perc', 'age_16_to_34_perc',\n                     'n_synergy', 'n_anchors', 'betweenness_centrality']\n\noutcome_profiles = X.copy()\noutcome_profiles['oof_outcome'] = oof_outcome.values\n\nprofile_summary = outcome_profiles.groupby('oof_outcome')[analysis_features].mean()\n\nprint(\"Mean Feature Values by OOF Prediction Outcome:\")\nprint(\"=\" * 70)\nprint(profile_summary.round(3).to_string())\nprint()\n\n# Highlight failure mode insights\nif 'FN' in profile_summary.index and 'TP' in profile_summary.index:\n    fn_vs_tp = profile_summary.loc['FN'] - profile_summary.loc['TP']\n    print(\"FN vs TP difference (what the model misses):\")\n    for feat in analysis_features:\n        diff = fn_vs_tp[feat]\n        direction = \"lower\" if diff < 0 else \"higher\"\n        print(f\"  {feat}: FN has {abs(diff):.3f} {direction} than TP\")\n\nprint()\nif 'FP' in profile_summary.index:\n    print(f\"False Positive count (OOF): {(oof_outcome == 'FP').sum()}\")\n    print(f\"False Negative count (OOF): {(oof_outcome == 'FN').sum()}\")\n    print(f\"True Positive count (OOF):  {(oof_outcome == 'TP').sum()}\")\n    print(f\"True Negative count (OOF):  {(oof_outcome == 'TN').sum()}\")\n\n# Visualise: grouped bar chart of feature means by outcome\nfig, ax = plt.subplots(figsize=(12, 6))\nprofile_norm = profile_summary.div(profile_summary.max())  # normalise for comparability\nprofile_norm.T.plot(kind='bar', ax=ax, width=0.8,\n                    color={'TP': '#e74c3c', 'FP': '#27ae60', 'TN': '#95a5a6', 'FN': '#f39c12'})\nax.set_ylabel('Normalised Mean Value', fontsize=11)\nax.set_title('Feature Profiles by Prediction Outcome (OOF)', fontsize=14, fontweight='bold')\nax.legend(title='Outcome', fontsize=10)\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 8c: Feature Importance — Tuned XGBoost\n",
    "# ============================================================\n",
    "importances = pd.Series(\n",
    "    best_model.feature_importances_, index=FEATURE_COLS\n",
    ").sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "importances.plot(\n",
    "    kind='barh', ax=ax,\n",
    "    color=['#e94560' if v > importances.median() else '#3498db' for v in importances]\n",
    ")\n",
    "ax.set_xlabel('Importance (Gain)', fontsize=12)\n",
    "ax.set_title('Feature Importance \\u2014 Tuned XGBoost', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=importances.median(), color='gray', linestyle='--', alpha=0.5, label='Median')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/outputs/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "print(importances.sort_values(ascending=False).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #27ae60; border-bottom: 2px solid #27ae60; padding-bottom: 10px;\">Section 9: Business Insight — Mining False Positives</h2>\n",
    "    <p>The most commercially valuable output of this model is not its accuracy — it is its <strong>mistakes</strong>.</p>\n",
    "    <p>A <strong>False Positive</strong> is a hexagon where:</p>\n",
    "    <ul>\n",
    "        <li>The model predicts <code>y=1</code> (this location <em>should</em> have a coffee shop)</li>\n",
    "        <li>The ground truth is <code>y=0</code> (no coffee shop currently exists here)</li>\n",
    "    </ul>\n",
    "    <p>These hexagons possess <em>all the learned features</em> of successful coffee shop locations —\n",
    "    high footfall, educated demographics, strong transit connectivity, synergy with gyms and offices —\n",
    "    but <strong>no one has opened a shop there yet</strong>.</p>\n",
    "    <p>In the language of network theory, these are <strong>Structural Holes</strong> (Burt, 1992) —\n",
    "    now identified not by heuristic scoring but by supervised machine learning.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SECTION 9: Extract False Positives as Site Recommendations\n# ============================================================\n\n# Retrain best model on full dataset for deployment predictions.\n# OOF evaluation was completed in Section 8; here we use the full-data\n# model to generate site recommendations (standard ML deployment practice).\nbest_model = grid_search.best_estimator_\nbest_model.fit(X, y)\n\ny_pred = best_model.predict(X)\ny_prob_best = best_model.predict_proba(X)[:, 1]\n\nh3_grid['predicted'] = y_pred\nh3_grid['actual'] = y.values\nh3_grid['predicted_prob'] = y_prob_best\n\n# Classify each hex by confusion matrix quadrant\ndef classify_outcome(row):\n    if row['predicted'] == 1 and row['actual'] == 0:\n        return 'False Positive (Recommendation)'\n    elif row['predicted'] == 1 and row['actual'] == 1:\n        return 'True Positive'\n    elif row['predicted'] == 0 and row['actual'] == 0:\n        return 'True Negative'\n    else:\n        return 'False Negative'\n\nh3_grid['outcome'] = h3_grid.apply(classify_outcome, axis=1)\n\nprint(\"Prediction Outcome Distribution:\")\nprint(h3_grid['outcome'].value_counts().to_string())\n\n# Extract and rank False Positives\nfp = h3_grid[h3_grid['outcome'] == 'False Positive (Recommendation)'].copy()\nfp_ranked = fp.nlargest(10, 'predicted_prob')\n\nprint(f\"\\n{'='*60}\")\nprint(f\"TOP 10 RECOMMENDED SITES (False Positives)\")\nprint(f\"{'='*60}\")\nfp_ranked[[\n    'h3_index', 'predicted_prob', 'population',\n    'level4_perc', 'age_16_to_34_perc',\n    'n_synergy', 'n_anchors', 'betweenness_centrality'\n]]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 9b: Demographic Profile of Recommended Sites\n",
    "# ============================================================\n",
    "# Compare FP sites vs Camden average\n",
    "profile_cols = ['population', 'level4_perc', 'age_16_to_34_perc',\n",
    "                'employed_total_perc', 'betweenness_centrality',\n",
    "                'n_synergy', 'n_anchors']\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Camden Average': h3_grid[profile_cols].mean(),\n",
    "    'Recommended Sites (FP)': fp[profile_cols].mean() if len(fp) > 0 else 0,\n",
    "    'Existing Shops (TP)': h3_grid[h3_grid['outcome'] == 'True Positive'][profile_cols].mean()\n",
    "})\n",
    "\n",
    "print(\"Demographic Profile Comparison:\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 30px;\">\n",
    "    <h2 style=\"color: #7d3c98; border-bottom: 2px solid #7d3c98; padding-bottom: 10px;\">Section 10: Interactive 3D Recommendation Map</h2>\n",
    "    <p>We visualise the full confusion matrix spatially using <strong>Pydeck</strong>:</p>\n",
    "    <ul>\n",
    "        <li style=\"color: #27ae60;\"><strong>Green (extruded)</strong>: False Positives — our site recommendations</li>\n",
    "        <li style=\"color: #e74c3c;\"><strong>Red (flat)</strong>: True Positives — existing coffee shops</li>\n",
    "        <li style=\"color: #95a5a6;\"><strong>Grey (flat)</strong>: True Negatives — not suitable</li>\n",
    "        <li style=\"color: #f39c12;\"><strong>Orange (flat)</strong>: False Negatives — missed by model</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 10: Pydeck 3D Recommendation Map\n",
    "# ============================================================\n",
    "viz_df = h3_grid.to_crs(epsg=4326).copy()\n",
    "\n",
    "# Colour coding by confusion matrix quadrant\n",
    "color_map = {\n",
    "    'False Positive (Recommendation)': [39, 174, 96, 200],   # Green\n",
    "    'True Positive':                   [231, 76, 60, 160],   # Red\n",
    "    'True Negative':                   [149, 165, 166, 80],  # Grey\n",
    "    'False Negative':                  [243, 156, 18, 160],  # Orange\n",
    "}\n",
    "\n",
    "viz_df['color'] = viz_df['outcome'].map(color_map)\n",
    "\n",
    "# Elevation: extrude FP sites by confidence, keep others flat\n",
    "viz_df['elevation'] = viz_df.apply(\n",
    "    lambda r: r['predicted_prob'] * 500\n",
    "    if r['outcome'] == 'False Positive (Recommendation)' else 10,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "layer = pdk.Layer(\n",
    "    'H3HexagonLayer',\n",
    "    viz_df,\n",
    "    pickable=True,\n",
    "    stroked=True,\n",
    "    filled=True,\n",
    "    extruded=True,\n",
    "    get_hexagon='h3_index',\n",
    "    get_fill_color='color',\n",
    "    get_elevation='elevation',\n",
    "    elevation_scale=1,\n",
    ")\n",
    "\n",
    "view_state = pdk.ViewState(\n",
    "    latitude=51.54, longitude=-0.14,\n",
    "    zoom=12.5, pitch=50, bearing=-15\n",
    ")\n",
    "\n",
    "tooltip = {\n",
    "    'html': (\n",
    "        '<b>H3:</b> {h3_index}<br>'\n",
    "        '<b>Outcome:</b> {outcome}<br>'\n",
    "        '<b>P(coffee):</b> {predicted_prob}<br>'\n",
    "        '<b>Population:</b> {population}<br>'\n",
    "        '<b>Level 4%:</b> {level4_perc}<br>'\n",
    "        '<b>Synergy:</b> {n_synergy} | <b>Anchors:</b> {n_anchors}'\n",
    "    ),\n",
    "    'style': {\n",
    "        'backgroundColor': '#1a1a2e',\n",
    "        'color': 'white',\n",
    "        'fontSize': '12px'\n",
    "    }\n",
    "}\n",
    "\n",
    "deck = pdk.Deck(layers=[layer], initial_view_state=view_state, tooltip=tooltip)\n",
    "deck.to_html('data/outputs/camden_ml_recommendations.html')\n",
    "\n",
    "print(\"Interactive 3D map saved: data/outputs/camden_ml_recommendations.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<div style=\"margin-top: 30px; padding: 20px; background-color: #1a1a2e; border-radius: 12px; border: 2px solid #e94560;\">\n    <h2 style=\"color: #e94560;\">Section 12: Model Card</h2>\n    <p style=\"color: #ccc;\">Following Mitchell et al. (2019), <em>Model Cards for Model Reporting</em>.</p>\n\n    <div style=\"background: #16213e; padding: 15px; border-radius: 8px; margin-top: 10px; color: #eee;\">\n        <h3 style=\"color: #e94560;\">1. Model Purpose</h3>\n        <p>Binary classifier identifying underserved H3 hexagonal cells for specialty coffee retail\n        in the London Borough of Camden. Intended users: B2B site selection analysts, commercial real\n        estate teams, and urban planning consultants.</p>\n        <p><strong>Use case:</strong> The model's False Positive predictions (FP) represent <em>structural holes</em>\n        &mdash; locations that possess the demand-side characteristics of existing coffee shop sites but\n        where no shop currently operates. These are candidate sites for new investment.</p>\n\n        <h3 style=\"color: #e94560; margin-top: 15px;\">2. Data Provenance</h3>\n        <table style=\"width:100%; color: #ccc; border-collapse: collapse; font-size: 0.9em;\">\n            <tr style=\"border-bottom: 1px solid #333;\">\n                <td style=\"padding: 6px;\"><strong>Population</strong></td>\n                <td>LandScan Global 2022 (Oak Ridge National Laboratory), ~1 km resolution GeoTIFF</td>\n            </tr>\n            <tr style=\"border-bottom: 1px solid #333;\">\n                <td style=\"padding: 6px;\"><strong>Demographics</strong></td>\n                <td>ONS Census 2021 (EDINA Digimap): age structure, economic activity, qualifications. Output Area level (~125 m median diameter in Camden)</td>\n            </tr>\n            <tr style=\"border-bottom: 1px solid #333;\">\n                <td style=\"padding: 6px;\"><strong>Points of Interest</strong></td>\n                <td>OpenStreetMap via OSMnx (snapshot at notebook runtime). Tags: amenity, leisure, shop, public_transport</td>\n            </tr>\n            <tr style=\"border-bottom: 1px solid #333;\">\n                <td style=\"padding: 6px;\"><strong>Spatial Unit</strong></td>\n                <td>Uber H3 Resolution 9 hexagons (~174 m edge length, ~0.105 km&sup2;)</td>\n            </tr>\n            <tr>\n                <td style=\"padding: 6px;\"><strong>Graph Features</strong></td>\n                <td>H3 adjacency graph via NetworkX. Degree, betweenness, closeness centrality + clustering coefficient</td>\n            </tr>\n        </table>\n\n        <h3 style=\"color: #e94560; margin-top: 15px;\">3. Evaluation Summary</h3>\n        <ul>\n            <li><strong>Best model:</strong> XGBoost (tuned via GridSearchCV)</li>\n            <li><strong>Evaluation protocol:</strong> 5-fold Spatial Block CV using H3 Res-5 parent partitioning</li>\n            <li><strong>Primary metric:</strong> ROC-AUC (out-of-fold predictions)</li>\n            <li><strong>Note:</strong> Spatial CV typically reduces apparent AUC by 5&ndash;15% compared to random CV due to controlling for spatial autocorrelation. This is a feature, not a bug &mdash; it reflects honest generalisation performance.</li>\n        </ul>\n\n        <h3 style=\"color: #e94560; margin-top: 15px;\">4. Known Limitations</h3>\n        <ul style=\"color: #ddd;\">\n            <li><strong>Geographic scope:</strong> Trained exclusively on Camden. The model may not generalise to other London boroughs or UK cities without retraining, as the feature&ndash;target relationship is conditioned on Camden's unique urban structure.</li>\n            <li><strong>Temporal snapshot:</strong> Census data is from 2021; OSM data from the notebook run date. The model does not capture neighbourhood evolution, new developments, or post-pandemic shifts in foot traffic.</li>\n            <li><strong>Binary target:</strong> Coffee shop presence &ne; profitability. A hexagon with a struggling caf&eacute; counts as positive (y=1). No revenue, footfall volume, or commercial rent data is incorporated.</li>\n            <li><strong>OSM completeness:</strong> OpenStreetMap coverage varies. Some POIs (especially offices and gyms) may be under-reported, affecting <code>n_synergy</code> accuracy.</li>\n            <li><strong>Omitted variables:</strong> Commercial rent, planning restrictions, lease availability, competitor brand strength, and pavement footfall counts are not modelled. The model identifies <em>demand-side</em> opportunity only.</li>\n            <li><strong>No held-out test set:</strong> All data is used within spatial CV. There is no temporally or geographically independent test set to validate deployment performance.</li>\n        </ul>\n\n        <h3 style=\"color: #e94560; margin-top: 15px;\">5. Ethical Considerations</h3>\n        <ul style=\"color: #ddd;\">\n            <li><strong>Gentrification risk:</strong> Recommending specialty coffee in underserved areas could contribute to gentrification pressure, potentially displacing existing businesses or communities. Decision-makers must consider local impact.</li>\n            <li><strong>Algorithmic bias:</strong> The model may encode demographic proxies. Areas with lower educational attainment are systematically ranked lower &mdash; this reflects market demand patterns but should not be interpreted as a normative judgement about community value.</li>\n            <li><strong>Human-in-the-loop:</strong> Predictions should <strong>inform</strong>, not replace, human decision-making. Field visits, stakeholder consultation, and financial due diligence must precede any investment decision.</li>\n        </ul>\n\n        <h3 style=\"color: #e94560; margin-top: 15px;\">6. Recommended Use</h3>\n        <p style=\"color: #ddd;\">This model is a <strong>shortlisting tool</strong>. It narrows the search space for site analysts\n        by identifying the most promising hexagonal zones. It must be combined with:\n        (a) field visits, (b) commercial rent analysis, (c) local authority planning checks, and\n        (d) community stakeholder consultation before any investment commitment.</p>\n    </div>\n</div>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 11: Export Final Results\n",
    "# ============================================================\n",
    "\n",
    "# 1. Full grid with predictions\n",
    "h3_grid.to_parquet('data/outputs/camden_ml_scored.parquet')\n",
    "\n",
    "# 2. Top 20 FP recommendations with lat/lon\n",
    "fp_top20 = h3_grid[\n",
    "    h3_grid['outcome'] == 'False Positive (Recommendation)'\n",
    "].nlargest(20, 'predicted_prob').copy()\n",
    "\n",
    "fp_export = fp_top20.to_crs(epsg=4326)\n",
    "fp_export['latitude'] = fp_export.geometry.centroid.y\n",
    "fp_export['longitude'] = fp_export.geometry.centroid.x\n",
    "\n",
    "export_cols = [\n",
    "    'h3_index', 'latitude', 'longitude', 'predicted_prob',\n",
    "    'population', 'level4_perc', 'age_16_to_34_perc',\n",
    "    'employed_total_perc', 'n_synergy', 'n_anchors',\n",
    "    'betweenness_centrality', 'closeness_centrality'\n",
    "]\n",
    "fp_export[export_cols].to_csv('data/outputs/fp_recommendations.csv', index=False)\n",
    "\n",
    "# 3. Model comparison summary\n",
    "results_df.to_csv('data/outputs/model_comparison.csv', index=False)\n",
    "\n",
    "print(\"Final outputs saved:\")\n",
    "print(\"  data/outputs/camden_ml_scored.parquet    (full grid + predictions)\")\n",
    "print(\"  data/outputs/fp_recommendations.csv       (top 20 site recommendations)\")\n",
    "print(\"  data/outputs/model_comparison.csv         (model AUC summary)\")\n",
    "print(\"  data/outputs/camden_ml_recommendations.html (interactive 3D map)\")\n",
    "print(\"  data/outputs/roc_curves.png               (ROC comparison chart)\")\n",
    "print(\"  data/outputs/confusion_matrix.png          (confusion matrix)\")\n",
    "print(\"  data/outputs/feature_importance.png        (feature importance chart)\")\n",
    "print(f\"\\nTotal False Positive recommendations: {len(fp)}\")\n",
    "print(\"Pipeline complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}